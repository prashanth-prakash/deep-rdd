{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A spiking discontinuity mechanism for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as rand\n",
    "import seaborn as sns\n",
    "\n",
    "from lib.lif import LIF_Recurrent, ParamsLIF_Recurrent\n",
    "from lib.causal import causaleffect_maxv, causaleffect_maxv_linear, causaleffect_maxv_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau_s = 0.020\n",
    "dt = 0.001\n",
    "t = 1\n",
    "DeltaT = 20\n",
    "\n",
    "t_filter = np.linspace(0, 1, 2000)\n",
    "exp_filter = np.exp(-t_filter/tau_s)\n",
    "exp_filter = exp_filter/np.sum(exp_filter)\n",
    "ds = exp_filter[0]\n",
    "\n",
    "params = ParamsLIF_Recurrent(exp_filter, dt = dt)\n",
    "lif = LIF_Recurrent(params, t = t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test simulation for fixed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(inp, v, h, u, sh, y) = lif.simulate(DeltaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot to see what's going on...\n",
    "#sh.shape\n",
    "#plt.imshow(sh[0,:,0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y = np.zeros(32).astype(int)\n",
    "#def sigma_prime(x):\n",
    "#    return (x>0).astype(float)\n",
    "#eta = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#self = lif\n",
    "#mean_activity = np.mean(sh, 2)\n",
    "#mean_inp = np.mean(inp,2)\n",
    "#hidden = mean_activity[:,0:self.params.n1]\n",
    "#output = mean_activity[:,self.params.n1:]\n",
    "#W = self.W\n",
    "#U = self.U[self.params.n1:, 0:self.params.n1]\n",
    "\n",
    "#y_hot = np.zeros((self.params.batch_size, self.params.n2))\n",
    "#for idx in range(self.params.batch_size):\n",
    "#    y_hot[idx,y[idx]] = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#e2 = np.multiply((y_hot - output), sigma_prime(np.matmul(U, hidden.T)).T)\n",
    "#e1 = np.multiply(np.matmul(e2, U), sigma_prime(np.matmul(W, mean_inp.T)).T)\n",
    "#gradU = np.matmul(e2.T, hidden)\n",
    "#gradW = np.matmul(e1.T, mean_inp)\n",
    "#U -= eta*gradU\n",
    "#W -= eta*gradW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.argmax(output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss = np.mean(np.power((y_hot - output),2)/2)\n",
    "#acc = 100*np.mean(np.argmax(output,1) == y)\n",
    "#acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Training loss: 0.048724, training accuracy: 9.375000\n",
      "Iteration: 1\n",
      "Training loss: 0.048697, training accuracy: 3.125000\n",
      "Iteration: 2\n",
      "Training loss: 0.048737, training accuracy: 12.500000\n"
     ]
    }
   ],
   "source": [
    "N = 3 #Number of iterations\n",
    "eval_every = 3\n",
    "for j in range(N):\n",
    "    print(\"Iteration: %d\"%j)\n",
    "    train_loss, train_acc = lif.train_BP(DeltaT)\n",
    "    print(\"Training loss: %f, training accuracy: %f\"%(train_loss, train_acc))\n",
    "\n",
    "#    if j % 3 == 0:\n",
    "#        loss, accuracy = lif.eval()\n",
    "#        print(\"Loss: %f, accuracy: %f\"%(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97.2667427 ,  98.92360218,  93.40882871,  79.57370703,\n",
       "        75.61029947,  72.21298354])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "np.random.uniform(10.5,100.5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-1,0,5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
